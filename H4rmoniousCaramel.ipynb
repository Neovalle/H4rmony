{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ-Zm5rCz3s1",
        "outputId": "27b2dc35-3bed-46eb-c388-2ec4ac28a3d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model H4rmoniousCaramel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"neovalle/H4rmoniousCaramel\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"neovalle/H4rmoniousCaramel\")"
      ],
      "metadata": {
        "id": "kAJziwdX-61J"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load H4rmoniousCaramel base (google/flan-t5-large)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
      ],
      "metadata": {
        "id": "PFuEvBd_nlpX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the prompt to test\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Answer the following prompt.\n",
        "\n",
        "For me, progress means turning forests into cities.\n",
        "\n",
        "Completion:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Miz3BsxojIk0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run test on base model\n",
        "inputs = base_tokenizer(prompt, return_tensors='pt')\n",
        "output = base_tokenizer.decode(\n",
        "    base_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'Base Model Output:\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niHPjeB8o95A",
        "outputId": "34bacb52-9a2d-4094-bdcc-6281bf56dcba"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model Output:\n",
            "is a goal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run test on H4rmony\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f'H4rmoniousCaramel Output:\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0pDylaJjaS_",
        "outputId": "9f19f866-50fc-4985-a0e7-b15baa55533b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H4rmoniousCaramel Output:\n",
            "The forest is the heart of our planet.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}