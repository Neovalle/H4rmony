{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "dataset1 = load_dataset(\"NickyNicky/neovalle_H4rmony_dpo_translated_English_to_Spanish\")['train']\n",
        "dataset2 = load_dataset(\"neovalle/H4rmony_dpo\")['train']\n",
        "\n",
        "# remove columns not to be merged\n",
        "dataset1 = dataset1.remove_columns(['prompt', 'chosen', 'rejected'])\n",
        "\n",
        "# Rename columns to match main dataset\n",
        "dataset1 = dataset1.rename_column('prompt_es', 'prompt')\n",
        "dataset1 = dataset1.rename_column('chosen_es', 'chosen')\n",
        "dataset1 = dataset1.rename_column('rejected_es', 'rejected')\n",
        "\n",
        "# Add \"lang\" column to both datasets by unpacking and adding new column and default value\n",
        "dataset1 = dataset1.map(lambda x: {**x, \"lang\": \"spa\"})\n",
        "dataset2 = dataset2.map(lambda x: {**x, \"lang\": \"eng\"})\n",
        "\n",
        "# Concatenate datasets\n",
        "merged_dataset = concatenate_datasets([dataset2, dataset1])\n",
        "\n",
        "# Convert to pandas DataFrame for display\n",
        "merged_df = pd.DataFrame(merged_dataset)\n",
        "\n",
        "# Save the merged DataFrame to a CSV file\n",
        "merged_df.to_csv('merged_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "avrDLhYNRK9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Dataframe to Dataset and push to hub\n",
        "\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('hf_token')\n",
        "\n",
        "# for multiple splits, create a DatasetDict\n",
        "# dataset_dict = DatasetDict({\"train\": dataset})\n",
        "\n",
        "# Push to the Hugging Face Hub\n",
        "#dataset_dict.push_to_hub(\"neovalle/neovalle/h4rmony_dpo_multilingual\") # multiple splits\n",
        "dataset.push_to_hub(\"neovalle/neovalle/h4rmony_dpo_multilingual\", token=hf_token)"
      ],
      "metadata": {
        "id": "zxKAjw6csTkq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}