{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is largely mlabonne's work:\n",
        "https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing\n",
        "\n",
        "with some minor changes \\to merge other other datesets and optional filtering."
      ],
      "metadata": {
        "id": "aQ0-84WFfkgV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4tKGKGGQWBH"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U transformers datasets accelerate peft trl bitsandbytes wandb --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set the environment"
      ],
      "metadata": {
        "id": "pxWbMH9hj3w7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwNxh0pCQhfI"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from google.colab import userdata\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
        "\n",
        "# Model\n",
        "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
        "new_model = \"neovalle/H4rmoniousOrpoLlama-3-8B\"\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "hf_token = userdata.get('hf_token')\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)\n",
        "\n",
        "# Set torch dtype and attention implementation\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install -qqq flash-attn\n",
        "    torch_dtype = torch.bfloat16\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "    torch_dtype = torch.float16\n",
        "    attn_implementation = \"eager\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kGBNStQoUd"
      },
      "outputs": [],
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model,token=hf_token)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=attn_implementation,token=hf_token\n",
        ")\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merging and Filtering"
      ],
      "metadata": {
        "id": "ovMalox4gqle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the first dataset\n",
        "dataset1 = load_dataset('mlabonne/orpo-dpo-mix-40k')\n",
        "\n",
        "# Load the second dataset\n",
        "dataset2 = load_dataset('neovalle/h4rmony_orpo')\n",
        "\n",
        "# filters, optional\n",
        "# dataset1 = dataset1.filter(lambda r: r[\"source\"] != \"toxic-dpo-v0.2\") # remove toxicity from orpo_dpo_mix\n",
        "# dataset2 = dataset2.filter(lambda x: x[\"lang\"] == \"eng\") # english only from h4rmony\n",
        "\n",
        "# Remove the unwanted column ('lang') from dataset2\n",
        "dataset2 = dataset2.remove_columns(['lang'])\n",
        "\n",
        "# Concatenate the cleaned datasets\n",
        "dataset = concatenate_datasets([dataset1['train'], dataset2['train']])\n"
      ],
      "metadata": {
        "id": "LsrbPb9tCLxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Format and Split"
      ],
      "metadata": {
        "id": "WmQE580iioV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaTou6EQQnAK"
      },
      "outputs": [],
      "source": [
        "\n",
        "#dataset = load_dataset(dataset_name, split=\"all\")\n",
        "#dataset = dataset.shuffle(seed=42).select(range(1000)) # Only use 1000 samples for quick demo\n",
        "\n",
        "def format_chat_template(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc= os.cpu_count(),\n",
        ")\n",
        "dataset = dataset.train_test_split(test_size=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set Hyperparameters and Train"
      ],
      "metadata": {
        "id": "mSu1m0lYi6iN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWDwJe7_Qqgb"
      },
      "outputs": [],
      "source": [
        "orpo_args = ORPOConfig(\n",
        "    learning_rate=8e-6,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    beta=0.1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=10,\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"./results/\",\n",
        ")\n",
        "\n",
        "trainer = ORPOTrainer(\n",
        "    model=model,\n",
        "    args=orpo_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the new model locally"
      ],
      "metadata": {
        "id": "X1iZwWEzjD-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(new_model)"
      ],
      "metadata": {
        "id": "zFOPyzu_BQbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge the adapters"
      ],
      "metadata": {
        "id": "NWJ_hPFHjNh4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6b3dCS6fL45"
      },
      "outputs": [],
      "source": [
        "# Flush memory\n",
        "del trainer, model\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reload tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n",
        "\n",
        "# Merge adapter with base model\n",
        "model = PeftModel.from_pretrained(fp16_model, new_model)\n",
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upload to Huggingface"
      ],
      "metadata": {
        "id": "GL_fDxyRjaw6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvWUrbyNgEnX"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}